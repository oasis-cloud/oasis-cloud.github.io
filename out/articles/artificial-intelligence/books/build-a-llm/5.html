<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>从零构建大模型（五）</title>
        <link rel="stylesheet" href="../../../../assets/fonts.css">
        <link rel="stylesheet" href="../../../../assets/graphite.css">
        <link rel="stylesheet" href="../../../../assets/pygments.css">
        <script src="../../../../assets/image-zoom.js" defer></script>
        
        
    </head>
    <body class="node-articles-artificial-intelligence-books-build-a-llm-5 node-articles-artificial-intelligence-books-build-a-llm node-articles-artificial-intelligence-books node-articles-artificial-intelligence node-articles node">
        <header class="masthead">
            <h1><a href="../../../../index.html">Oasis's Cloud</a></h1>
            
                <p class="tagline">一个人的首要责任，就是要有雄心。雄心是一种高尚的激情，它可以采取多种合理的形式。<br />—— 《一个数学家的辩白》</p>
            
            
            <nav class="menu">
                <input id="menu-check" type="checkbox"/>
                <label id="menu-label" for="menu-check" class="unselectable">
                    <span class="icon close-icon">✕</span>
                    <span class="icon open-icon">☰</span>
                    <span class="text">Menu</span>
                </label>
                <ul>
<li><a href="../../../../index.html">首页</a></li>
</ul>
            </nav>
        </header>
        <article class="main">
            <header class="title">
                <h1>从零构建大模型（五）</h1>
                
                    <p class="subtitle">在无标签数据上进行预训练</p>
                
                
                    <p class="author">作者：oasis</p>
                
                <hr>
            </header>
            <h2>模型评估</h2>
<p>模型评估主要围绕 损失值 这个概念展开。损失值将作为训练进展和成功的衡量标准。而训练大语言模型的目标是最大化正确词元的可能性，也就是最大化与目标词元对应的 softmax 概率值。</p>
<p>通过更新模型权重，可以最大化与目标词元对应的 softmax 概率值。权重更新是通过一种称为反向传播的过程完成的。反向传播需要一个损失函数（计算模型的预测输出与实际期望输出之间的差异）。
这个损失函数衡量的是模型的预测与目标值之间的偏差。</p>
<p>文本生成损失的计算逻辑：</p>
<ol>
<li>logits</li>
<li>概率</li>
<li>目标概率</li>
<li>对数概率</li>
<li>平均对数概率</li>
<li>负平均对数概率</li>
</ol>
<p>模型的评估主要体现在负平均对数概率，在深度学习中，通常的做法是将负平均对数概率降至 0。</p>
<p>交叉熵损失用于衡量两个概率分布之间的差异——通常是标签（数据集中的词元）的真是分布和模型生成的预测分布之间的差异。</p>
<p>困惑度可以衡量模型预测的概率分布与数据集中实际词汇分布的匹配程度。与损失类似，较低的困惑度表明模型的预测更接近实际分布。困惑度实际表示了模型不确定在词汇表的N个词元中应该生成哪个来作为下一个词元。这样的方式更容易解释。</p>
<p>本书中先设计了衡量的方法，这很值的借鉴，就像在实际开发中，先编写测试用例，才能以目标为导向的走下去。</p>
<h2>训练 LLM</h2>
<p>此书中实现了简单的训练循环。首先需要将数据分成：训练集和验证集。书中采用了 90% 的数据进行训练，10% 的作为验证数据。</p>
<p>分块和批次在训练循环中的所指，可参考下面的例子：</p>
<div class="highlight"><pre><span></span><code>文本 = &quot;深度学习让计算机能够理解人类语言。人工智能正在改变世界。&quot;
</code></pre></div>
<p>分词：
<div class="highlight"><pre><span></span><code>分词后 = [&quot;深度&quot;, &quot;学习&quot;, &quot;让&quot;, &quot;计算机&quot;, &quot;能够&quot;, &quot;理解&quot;, &quot;人类&quot;, &quot;语言&quot;, &quot;。&quot;, 
         &quot;人工&quot;, &quot;智能&quot;, &quot;正在&quot;, &quot;改变&quot;, &quot;世界&quot;, &quot;。&quot;]
</code></pre></div></p>
<p>分块（块大小=6）
<div class="highlight"><pre><span></span><code>块1: [&quot;深度&quot;, &quot;学习&quot;, &quot;让&quot;, &quot;计算机&quot;, &quot;能够&quot;, &quot;理解&quot;]
块2: [&quot;人类&quot;, &quot;语言&quot;, &quot;。&quot;, &quot;人工&quot;, &quot;智能&quot;, &quot;正在&quot;]
块3: [&quot;改变&quot;, &quot;世界&quot;, &quot;。&quot;, &quot;<span class="p">&lt;</span><span class="nt">填充</span><span class="p">&gt;</span>&quot;, &quot;<span class="p">&lt;</span><span class="nt">填充</span><span class="p">&gt;</span>&quot;, &quot;<span class="p">&lt;</span><span class="nt">填充</span><span class="p">&gt;</span>&quot;]
</code></pre></div></p>
<p>分批（批次大小=2）
<div class="highlight"><pre><span></span><code>批次1: [块1, 块2]
批次2: [块3, <span class="p">&lt;</span><span class="nt">空块</span><span class="p">&gt;</span>]
</code></pre></div></p>
<h2>解码策略（文本生成策略）</h2>
<p>之所以引入解码策略，是因为生成的词元是从词汇表的所有词元中选择概率分数最大的那一个，这样即使在相同的起始上下文中多次运行前面的生成函数，大语言模型将始终生成相同的输出。</p>
<p>下面有两种算法可以让生成的文本更有原创性：</p>
<ol>
<li>温度缩放：在下一个词元生成任务中添加概率选择过程的技术。温度缩放指的是将 logits 除以一个大于 0 的数。</li>
<li>Top-k采样：其目的是解决温度缩放下可能导致的语法不正确或完全无意义的输出。在 Top-k 采样中，可以将采样的词元限制在前 K 个最可能的词元上，并通过掩码概率分数的方式来排除其他词元。</li>
</ol>
<p>Top-k 采样像编译器的词法分析器向前看 k 个 Token。 在生成文本函数的中先应用 Top-k 采样，然后在通过温度缩放，确定下一个词元。</p>
<h2>加载和保存权重</h2>
<p>模型的训练耗时耗资源，因此本地化模型权重就变得很重要。在 PyTorch 中通过 save 方法可以保存模型的 state_dict。文件扩展名为 <code>.pth</code></p>
<p>既然可以本地化模型的 state_dict，那么应该有对应的加载方法。</p>
        </article>
        
        <script src="https://giscus.app/client.js"
                data-repo="oasis-cloud/blog-comments"
                data-repo-id="R_kgDOKEliHA"
                data-category="General"
                data-category-id="DIC_kwDOKEliHM4CYb6e"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="preferred_color_scheme"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        </script>
    </body>
</html>
